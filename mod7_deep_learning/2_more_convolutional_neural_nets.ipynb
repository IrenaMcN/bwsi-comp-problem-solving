{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ed6989",
   "metadata": {},
   "source": [
    "# Deep Learning: More Convolutional Neural Networks\n",
    "\n",
    "Welcome back! Today‚Äôs small‚Äìgroup lecture explores **Convolutional Neural Networks (CNNs)**, building directly on what we've learned so far in deep learning.\n",
    "\n",
    "CNNs are the workhorse behind many modern computer vision systems ‚Äî from image classification (e.g., recognizing cats vs. dogs) to object detection (e.g., detecting pedestrians in self-driving cars) and even style transfer.\n",
    "\n",
    "In a standard fully-connected neural network, every neuron in one layer connects to every neuron in the next. That works well for tabular data, but it ignores the fact that **images have spatial structure**: nearby pixels tend to be related. CNNs exploit this by using small filters (kernels) that slide over the image to detect local patterns like edges and textures.\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Understand the intuition behind CNNs and why they work so well for image data.\n",
    "- Understand the syntax used to write a multi-layer PyTorch CNN.\n",
    "- Explain the role of activation functions, batch normalization, and max pooling.\n",
    "- Train, validate, and analyze a CNN‚Äôs performance.\n",
    "- Understand core training concepts such as optimizers, loss functions, epochs, and learning rate.\n",
    "\n",
    "This lecture is designed to feel like a guided walkthrough ‚Äî with explanations of not just **what** we do, but **why** we do it. As you go, try to connect the code to the mathematical ideas from lecture.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "147a4157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.9.1+cu128\n",
      "Torchvision version: 0.24.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f632b4",
   "metadata": {},
   "source": [
    "### Note: Python must be 3.11.x\n",
    "\n",
    "If your kernel shows a value > 3.11.x you will need to downgrade. Please email the staff for help.\n",
    "\n",
    "**Why does this matter?** Deep learning frameworks like PyTorch provide precompiled binaries (called *wheels*) for specific Python versions and hardware setups. When Python introduces a new major/minor version (like 3.12), the internal binary interface (ABI) can change. If PyTorch hasn't released a compatible wheel yet, you may see runtime errors, crashes, or import failures.\n",
    "\n",
    "So as a rule of thumb:\n",
    "- Always check the PyTorch install page for supported Python versions.\n",
    "- If you see version mismatches, try creating a **conda or venv environment** with Python 3.11 specifically for deep learning work.\n",
    "\n",
    "In this class, we standardize on **Python 3.11.x** so the environment is reproducible and supportable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc40539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# DEVICE CONFIGURATION\n",
    "if torch.backends.mps.is_available():          # Apple Silicon (Metal Performance Shaders)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():                # NVIDIA GPU with CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")               # Fallback to CPU\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dc771",
   "metadata": {},
   "source": [
    "For today's small group, we will walk through the process of setting up a convolutional neural network (\"CNN\" for short) using the `pytorch` package!\n",
    "\n",
    "CNNs shine when working with **structured grid data**, especially images ‚Äî tasks like classification, segmentation, object detection, and more.\n",
    "\n",
    "Why? Because CNNs:\n",
    "- Capture local patterns (edges, textures) via **convolutions**.\n",
    "- Build hierarchical features (shapes ‚Üí object parts ‚Üí full objects) through **stacked layers**.\n",
    "- Use **shared weights**, meaning the same filter is applied across the whole image, which makes them parameter-efficient and **translation-equivariant** (shifting the image shifts the feature map in a predictable way).\n",
    "\n",
    "### Quick Mental Picture üñºÔ∏è\n",
    "- Early layers learn to detect **edges** and simple textures.\n",
    "- Middle layers detect **parts** like eyes, wheels, or wings.\n",
    "- Later layers detect **semantic concepts** like \"dog\", \"car\", or \"bird\".\n",
    "\n",
    "Let‚Äôs load a dataset so we can see these ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677b0b9",
   "metadata": {},
   "source": [
    "Recall from lecture that CNNs are generally used to process gridded data or images.\n",
    "\n",
    "Let's begin by loading one of the toy datasets included in `pytorch`: **CIFAR-10**.\n",
    "\n",
    "The dataset contains 60,000 small 32√ó32 color images belonging to 10 different classes:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "There are 50,000 training images and 10,000 test images. Each image has **3 color channels (RGB)** and a relatively low resolution (32√ó32), which makes it great for teaching and small experiments.\n",
    "\n",
    "We will also apply a **transform pipeline** to preprocess the images before feeding them into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dba6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init preprocessing for CIFAR-10 dataset (images are 32x32x3)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # ToTensor() converts images from [0, 255] uint8 to [0.0, 1.0] float32\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize each channel to roughly [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6085fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a204a",
   "metadata": {},
   "source": [
    "Great! We have image data now. But what does it look like?\n",
    "\n",
    "Visualizing your data is an essential first step ‚Äî especially in computer vision. It's very easy to accidentally:\n",
    "- Misinterpret labels.\n",
    "- Apply the wrong preprocessing.\n",
    "- Feed the network images that are rotated, upside-down, or poorly scaled.\n",
    "\n",
    "Let's plot the different classes below using `matplotlib`. When working with the visualizations, consider:\n",
    "- *What patterns do you notice?* (e.g., background colors, viewpoints, clutter)\n",
    "- *Which classes might be harder for the network? Why?* (e.g., cat vs. dog may be harder than airplane vs. frog)\n",
    "\n",
    "You are encouraged to:\n",
    "- Sample a batch from `train_loader`.\n",
    "- Show a grid of images.\n",
    "- Print the corresponding class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e61a08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py311/bin/python\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import numpy as np\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a300eff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Plot each class here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m images, labels = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Only changes: move to CPU and detach\u001b[39;00m\n\u001b[32m      5\u001b[39m images = images.cpu().detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torchvision/datasets/cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py311/lib/python3.11/site-packages/torchvision/transforms/functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Plot each class here\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Only changes: move to CPU and detach\n",
    "images = images.cpu().detach()\n",
    "labels = labels.cpu().detach()\n",
    "\n",
    "classes = train_dataset.classes\n",
    "fig, axes = plt.subplots(3, 4, tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(12):\n",
    "    # Convert to NumPy for matplotlib\n",
    "    axes[i].imshow(images[i].permute(1,2,0).numpy())\n",
    "    axes[i].set_title(classes[int(labels[i])])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d308dd",
   "metadata": {},
   "source": [
    "## Building a CNN\n",
    "\n",
    "Conceptually, our CNN will:\n",
    "1. Take a 3√ó32√ó32 image.\n",
    "2. Apply a series of convolution + nonlinearity + pooling operations that gradually reduce spatial size but increase the number of feature maps.\n",
    "3. Flatten the final feature maps into a vector.\n",
    "4. Feed that vector into a fully connected layer that outputs **logits** for the 10 CIFAR-10 classes.\n",
    "\n",
    "The figure below shows a high-level diagram of a CNN:\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250529121802516451/Convolutional-Neural-Network-in-Machine-Learning.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce **nonlinearity** into the network, which allows the model to approximate complex functions.\n",
    "\n",
    "Some common activation functions:\n",
    "\n",
    "- **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)`\n",
    "  - Very simple, fast to compute.\n",
    "  - Avoids vanishing gradients on the positive side.\n",
    "  - TensorFlow equivalents: `tf.nn.relu` or `tf.keras.layers.ReLU()`.\n",
    "\n",
    "- **LeakyReLU**: `f(x) = x` if `x > 0`, and `Œ±x` otherwise (Œ± is small, like 0.01).\n",
    "  - Solves the \"dying ReLU\" problem by allowing a small negative gradient.\n",
    "  - TensorFlow: `tf.nn.leaky_relu`.\n",
    "\n",
    "- **Tanh**: squashes values to (-1, 1).\n",
    "  - Zero-centered, which can help optimization.\n",
    "  - Still suffers from saturation/vanishing gradients at large |x|.\n",
    "  - TensorFlow: `tf.nn.tanh` or `tf.keras.activations.tanh`.\n",
    "\n",
    "Without nonlinear activations, a stack of layers collapses to a single linear transformation ‚Äî so no matter how many layers you add, the overall function is still just a linear mapping. Nonlinear activations give the network the ability to approximate **complex nonlinear functions**.\n",
    "\n",
    "Below is a simple CNN model in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11039ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input shape: (batch_size, 3, 32, 32)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),        # -> (batch_size, 32, 32, 32)\n",
    "            nn.BatchNorm2d(32),                                # normalize activations per channel\n",
    "            nn.ReLU(),                                         # nonlinear activation\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 32, 16, 16)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),       # -> (batch_size, 64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 64, 8, 8)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),      # -> (batch_size, 128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 128, 4, 4)\n",
    "        )\n",
    "        # Flattened feature vector size: 128 * 4 * 4 = 2048\n",
    "        self.fc = nn.Linear(128*4*4, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)  # flatten (batch_size, 128*4*4)\n",
    "        out = self.fc(out)               # output logits (batch_size, 10)\n",
    "        return out\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2320e",
   "metadata": {},
   "source": [
    "### Q: What do you think will happen to your CNN as you change the activation function?\n",
    "Try experimenting by changing the activation layers in `CNN` and observing training and test accuracy.\n",
    "\n",
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf767de",
   "metadata": {},
   "source": [
    "----\n",
    "## Training a CNN \n",
    "\n",
    "Training a neural network is an instance of **iterative optimization**. \n",
    "\n",
    "The key components are:\n",
    "\n",
    "1. **Loss function (objective)**: tells us how bad our predictions are.\n",
    "2. **Optimizer**: decides how to update parameters to reduce the loss.\n",
    "3. **Learning rate**: controls how big each update step is.\n",
    "4. **Epochs and batches**: define how many passes we make over the dataset and how we break data into mini-batches.\n",
    "\n",
    "### Loss function\n",
    "- For multi-class classification like CIFAR-10, we treat the model‚Äôs output as **logits** (unnormalized scores) for each class and compare them to the correct class labels.\n",
    "- `nn.CrossEntropyLoss()` in PyTorch:\n",
    "  - Applies a softmax internally to convert logits to probabilities.\n",
    "  - Computes the negative log-likelihood of the correct class.\n",
    "  - Encourages the network to assign high probability to the correct label.\n",
    "\n",
    "Other loss functions (for context):\n",
    "- `nn.MSELoss()` (mean squared error): often used in regression, not ideal for classification because it doesn‚Äôt match the probabilistic structure of the problem.\n",
    "- `nn.BCEWithLogitsLoss()`: used for binary classification or multi-label classification (where each class is independently 0 or 1).\n",
    "\n",
    "### Optimizer\n",
    "- **SGD (Stochastic Gradient Descent)**:\n",
    "  - Update rule: `Œ∏ ‚Üê Œ∏ ‚àí Œ∑ * ‚àá_Œ∏ L` (plus optional momentum).\n",
    "  - Simple and effective but can require careful tuning of the learning rate and momentum.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**:\n",
    "  - Keeps moving averages of both gradients and squared gradients.\n",
    "  - Adapts the step size for each parameter separately.\n",
    "  - Often converges faster with less manual tuning, making it a great default choice.\n",
    "\n",
    "### Learning rate\n",
    "- The learning rate `Œ∑` determines the **step size** in parameter space.\n",
    "- Too large: the loss can oscillate or blow up, because we overshoot the minimum.\n",
    "- Too small: the network learns extremely slowly and may appear stuck.\n",
    "- In practice, people often use **learning rate schedules** (decay over time) or **adaptive optimizers** like Adam.\n",
    "\n",
    "[This](https://www.geeksforgeeks.org/machine-learning/epoch-in-machine-learning/) reference will be useful to review the concept of epochs if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327ce76",
   "metadata": {},
   "source": [
    "### Q: What might happen if we changed our loss from __ to __ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217ce9c",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38868e01",
   "metadata": {},
   "source": [
    "### Q: What happens if the `learning_rate` parameter is too high? Or too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c969a87",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0828170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Note: an early stopping condition could be added here based on validation loss.\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()  # set model to training mode (enables dropout, batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()   # clear old gradients\n",
    "        loss.backward()         # compute new gradients via backprop\n",
    "        optimizer.step()        # update model parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute training accuracy for this batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_total += labels.size(0)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Epoch-level metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_train_acc = 100.0 * running_correct / running_total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # Evaluate on test data at the end of the epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    epoch_test_acc = 100.0 * correct / total\n",
    "    test_accuracies.append(epoch_test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, Test Acc: {epoch_test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303225d",
   "metadata": {},
   "source": [
    "### Q: What happens if you increase `epochs`? Will performance always improve as `epochs` increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faaaf70",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f05bd",
   "metadata": {},
   "source": [
    "----\n",
    "## Validating a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d4cdd",
   "metadata": {},
   "source": [
    "We now want to evaluate how well our trained model **generalizes** to unseen data. This is why we kept a separate **test set**.\n",
    "\n",
    "Key steps during evaluation:\n",
    "- Call `model.eval()` to set the model to **evaluation mode**.\n",
    "  - This tells layers like BatchNorm and Dropout to behave differently (e.g., BatchNorm uses running statistics instead of batch statistics, Dropout is disabled).\n",
    "- Use `torch.no_grad()` to avoid tracking gradient computations.\n",
    "  - This saves memory and speed, since we aren‚Äôt going to backpropagate.\n",
    "- Loop over the `test_loader`, compute predictions, and count how many are correct.\n",
    "\n",
    "The result is the **test accuracy**, which we treat as an estimate of how well our model will perform on new, real-world data from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb98ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final testing pass (optional if you already evaluate each epoch)\n",
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # torch.max returns (max_value, index_of_max)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Final Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff15a83",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34171286",
   "metadata": {},
   "source": [
    "## Analyzing Performance\n",
    "\n",
    "A single test accuracy number is useful, but it doesn‚Äôt tell the whole story. Often, we want to understand **how performance changes over time** as we train longer.\n",
    "\n",
    "Things to consider when looking at an accuracy-vs-epochs plot:\n",
    "- Does training accuracy keep going up while test accuracy plateaus? ‚Üí Likely overfitting.\n",
    "- Are both training and test accuracy low and not improving? ‚Üí Underfitting (model too simple, learning rate too low, not enough epochs, etc.).\n",
    "- Do we see noisy, jagged curves? ‚Üí Maybe a high learning rate or small batch size.\n",
    "\n",
    "You can also:\n",
    "- Plot both **training loss** and **test accuracy** vs. epoch.\n",
    "- Check if there is a gap between training and test curves.\n",
    "- Use such plots to decide when to stop training or when to adjust hyperparameters.\n",
    "\n",
    "In the cell below, we assume that `train_losses`, `train_accuracies`, and `test_accuracies` were populated during training and use them to create plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91954382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss vs. epochs\n",
    "if 'train_accuracies' in globals() and 'test_accuracies' in globals() and 'train_losses' in globals():\n",
    "    epochs = range(1, len(train_accuracies) + 1)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_accuracies, marker='o', label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies, marker='s', label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Train vs Test Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history found. Please run the training cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
